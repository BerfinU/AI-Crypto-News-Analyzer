{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzqm6aOE_f3c"
   },
   "outputs": [],
   "source": [
    "!pip install \"transformers==4.53\" datasets>=2.14 pandas>=2.0 torch>=2.6.0 scikit-learn>=1.3 numpy>=1.24 matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "459417266f104073924368e9f78ceb89",
      "f7bf20df52274b32b34cc17731f7d456",
      "4e42fd08dfa142048f524045d3f0c6a0",
      "d83888726b814cce8ad644035d57ecf2",
      "6906236505384d6bb92dbfedc608e77f",
      "3b072910e1ac492d86dded194342aadf",
      "1ed79f0d038444d1ab2ea1cd19a38d3c",
      "46f50f58aeac43db8d5e27a5bed8ec49",
      "226ff7503a1c46a8849ed5c9b6e0073b",
      "3db15e1bbe014d558f161653bcd0e19c",
      "8b175e0a8c764dad803199bc0081b73a",
      "e7b5e0eb90854ac098d6f9eb1f12a208",
      "464156c954a54d9a8a25441a7774e163",
      "439ac56844d140089d55cc92d0e2c741",
      "9d5cc7795ca445c6b7e271f55183808c",
      "56ac1b2f1e964bd194142806e8cac281",
      "b3fe1542eafe4195a9c2cb4cf8b262ed",
      "adeaf48963b142b2949adb09b1f41dac",
      "7d41cd2e27ca402c98cce19de75ac1fe",
      "1dafad8657fe4b73967f28439a9e9786",
      "06e56172393e48bba0ee2e906ed0f309",
      "3fde0b8f4f92436d8e0ad4b87333a22b",
      "582446f9bef1442fb324435dd78c8072",
      "efedc7effab54a73ba0d1ad64bb9e99e",
      "06f60e1dd9d24ca096c7f71146c9acde",
      "0ecb3aa435eb45fb943efa60398771c4",
      "8994694dc2f947459bb0a0352071abc4",
      "df8d15e108154b36920242a4c504207d",
      "4a3b787a3e2a45e2b164ffdd9f0dddc2",
      "c036d53e6ec745d98d732587073ea045",
      "3286d281d55643da8c250c5d5e0b4c5b",
      "966b34ca670c4ae0bc8ef86e2d92a38c",
      "43b68ce4d2204e9ca238838cc26b6dc0",
      "df3aeaf1c43d44c4a2a0588f699a5fbe",
      "a14753f2e6af4d63818e2e0ac1ba580b",
      "d59dbf0f86774d58a02eed028fa2bb8d",
      "d6ea78926f20471cadf95345ce99cf43",
      "a2847d2ccdcc4f54833dff88558d8c08",
      "98a3bc047f2a43d8a43e748db276493e",
      "9fef41779d4d44c1a0cd3f949b291b29",
      "3a5a07a3f326491e930cac1659f0f224",
      "3fb998d6bce64ec9b7c3d6d5b8286bd1",
      "0a70f020d75f464daa0b11217b1a8ad9",
      "268133cbc2694371b3c6e10a72fc8afd",
      "ba7cc20ed64a43729e0ecdd7a9c0be32",
      "c4ed4eca8d5441d09a7ff53a5f4d5e5d",
      "10fa8c79bab341afa3c3458f5ddf39e2",
      "02057c6fe0574b3a92bf1faa39303905",
      "83787b292a5c43429a69348bdae3d578",
      "eebe6cfb94344b039f6c87e2a280a7e2",
      "d658521cfdf04c72a275d0c03d798e96",
      "178af1db3c614ce7a188e5805438f672",
      "af12d698f1c547069850fb37bd00f804",
      "e897eabe0668485baeae0b455e24bb19",
      "6246da6c9d9241faac1d11c3a0adbf56",
      "448e8f4ba0ed4ca19fe60bfc5a2da047",
      "e7e2657b644349cc879ceaff6de61633",
      "fac4f1d218f2436fb77298d8ba083c45",
      "79d8e3e0c5174944a5c55616c0b892e7",
      "232566673d614347954a1b70c8aa0b16",
      "ecb169e7a832472cb11eefe2f3acc263",
      "11559c5172ae4819abc0d2cd15eb21b3",
      "e9e030d84a2f422ba206b6125a849c45",
      "fb19169c08ed4926bbc20b9ff8f9eca7",
      "2c834b0727f14b8fa0d3e7aea0a70a77",
      "542d0e29d5de4d00b143ef98d8185c44",
      "1ad80362ce7244c1a3a30a7c69b70dcd",
      "aee0bd3e88a74890bdd52a27f52b199c",
      "2459116da0fb4f15b2d2534a279b04ca",
      "2fce9a54f22b4217a4e48db54bed309c",
      "5635349d0cfe441d9e256ab1de540ddc",
      "215a4dfe071948e7adf490ed78b7cb5f",
      "81b3f77cbb3e46a9980d78f4a3f94823",
      "89440582104544a7b4aa62a2afd28136",
      "e2508e38d0e94874b20378440f6f6a08",
      "dc820abe01d5484ca14c0bac380a2119",
      "53eade224a6d483093a4ae0bcbacb16e",
      "c350a7f28ba44821b29a804771467bcd",
      "2396b377cc954d5e854ad09b9c66b353",
      "92a5c97be7544881bef17ac24d28c483",
      "5e34184fbb2045ccaf8bcfa6b37e579f",
      "30e025ac6cc94f5486d8cb5830848ee1",
      "da29df345aa3457ebde4f11f991d4da9",
      "4cefa844d9d740dab0a91eb750c94b68",
      "9cdff67c298d4fad9fee52629845239f",
      "273e5b9637184fe798caabd152f18aaf",
      "72c63157b9154ab6bc0d097281c73fe4",
      "8783e6639f3f491dad89df8c6c7372d9",
      "7e67ab5a9e424947bc3238a66fce904a",
      "760a5ac07ceb4a858fc5bb27bbbb4c35",
      "fd507238fde64129b0ed416e870fcdff",
      "b3f52c3c374c4b55a67efd95ba3c5394",
      "209b0d2045bc403e8eafb1a5fb0f3fef",
      "7b96a3854e564ad685ff908f574571a7",
      "69e53c1923df4d97b359e664aff580ef",
      "e8eff6643e6045498c1a61f2589ad3fa",
      "a092b1ed1f2e49d6bfa5d0aa454cf864",
      "ed45896aa36e4308bca7f944097ac07a",
      "e38cb749aadc4c9eb5729b7b6d2a0c48",
      "88021ca9bfb346719899d62a39aa498a",
      "7feba9cd6f2d4e18981d2f29c700c9b3",
      "1735dee496f7432bb76e608b40c4fd73",
      "1d1f038f1fee479bbeefbafcc28cf9a9",
      "622ee29bfef74e45bb4b832003c3a4f1",
      "028337f8d42a4bb5a14fc28ee7d2806a",
      "7a288d8eef5946e492f7ae7e643a5094",
      "c4de298d9bd4473e89a122eda49357c3",
      "6e20ecf9e8f9403baa646de3ec2f1ad6",
      "ff13027a65c64134a753acec788609bd",
      "d9d64cc7a9ec4e78ac5808e38a2b33ed",
      "12fea67b4eba4289aeeac14109bdb7f9",
      "15e3aad781d2417098e18bcfecb5561b",
      "824def2917484305bfc3e91a1fb7acfc",
      "bf343d4df07d49f88c0b38d66a598d3d",
      "58a0e503087c4bcd935aa8701a1e970d",
      "419ce2ff609e4481bf11a38320c96cf1",
      "d28692146f3845c38883e84092c7f852",
      "455c9b99b2dc4d6dba3a01c62ac7e018",
      "8fde0a81b4374092b7499993280b7790",
      "54230432a3624cf6ae8355612afb72da",
      "05cd01e46c8144548256abc18185d4f7"
     ]
    },
    "id": "WzKHO4PtRs6C",
    "outputId": "f0ab3b6a-801a-4ff1-d9ed-cc2306943318"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForSequenceClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorWithPadding,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import zipfile\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class OptimizedConfig:\n",
    "    \"\"\"1835 dataset için optimize edilmiş configuration.\"\"\"\n",
    "\n",
    "    BASE_MODEL_NAME = \"bert-base-uncased\"\n",
    "    MODEL_SAVE_PATH = \"/content/crypto_risk_classifier_bert\"\n",
    "\n",
    "    # optimize edilmiş hyperparameters\n",
    "    BATCH_SIZE = 16\n",
    "    LEARNING_RATE = 2e-5\n",
    "    NUM_EPOCHS = 8\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    DROPOUT_RATE = 0.2\n",
    "    WARMUP_RATIO = 0.1\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    ENHANCE_PATTERNS = True\n",
    "    USE_CLASS_WEIGHTS = False\n",
    "\n",
    "    LABEL_NAMES = [\n",
    "        \"Important\",\n",
    "        \"Medium\",\n",
    "        \"Unimportant\"\n",
    "    ]\n",
    "\n",
    "    LABEL_TO_ID = {label: idx for idx, label in enumerate(LABEL_NAMES)}\n",
    "    ID_TO_LABEL = {idx: label for label, idx in LABEL_TO_ID.items()}\n",
    "\n",
    "config = OptimizedConfig()\n",
    "\n",
    "def get_device():\n",
    "    \"\"\"Device selection optimized for 1835 dataset.\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"GPU detected: {gpu_name} ({gpu_memory:.1f}GB)\")\n",
    "\n",
    "        if gpu_memory >= 16:\n",
    "            config.BATCH_SIZE = 24\n",
    "        elif gpu_memory >= 8:\n",
    "            config.BATCH_SIZE = 16\n",
    "        else:\n",
    "            config.BATCH_SIZE = 12\n",
    "\n",
    "        print(f\"Batch size set to: {config.BATCH_SIZE}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU (training will be slower)\")\n",
    "        config.BATCH_SIZE = 8\n",
    "\n",
    "    return device\n",
    "\n",
    "def enhance_text_with_patterns(text: str, label: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Enhance text with pattern indicators that help model learn\n",
    "    what makes news Important/Medium/Unimportant\n",
    "    \"\"\"\n",
    "    if not config.ENHANCE_PATTERNS:\n",
    "        return text\n",
    "\n",
    "    enhanced_text = text\n",
    "\n",
    "    financial_patterns = {\n",
    "        'billions': r'\\$?\\d+\\.?\\d*\\s*billion|\\$?\\d+\\.?\\d*B',\n",
    "        'millions': r'\\$?\\d+\\.?\\d*\\s*million|\\$?\\d+\\.?\\d*M',\n",
    "        'thousands': r'\\$?\\d+\\.?\\d*\\s*thousand|\\$?\\d+\\.?\\d*K'\n",
    "    }\n",
    "\n",
    "    regulatory_keywords = [\n",
    "        'SEC', 'regulatory', 'approval', 'lawsuit', 'guilty', 'fine',\n",
    "        'government', 'legal', 'court', 'ruling', 'banned', 'regulation'\n",
    "    ]\n",
    "\n",
    "    market_impact_keywords = [\n",
    "        'market', 'price', 'surge', 'crash', 'rally', 'dump',\n",
    "        'all-time high', 'ATH', 'bear market', 'bull market'\n",
    "    ]\n",
    "\n",
    "    security_keywords = [\n",
    "        'hack', 'exploit', 'vulnerability', 'breach', 'stolen',\n",
    "        'scam', 'fraud', 'attack', 'security'\n",
    "    ]\n",
    "\n",
    "    major_entities = [\n",
    "        'Bitcoin', 'Ethereum', 'Binance', 'Coinbase', 'Tesla',\n",
    "        'MicroStrategy', 'BlackRock', 'Grayscale', 'PayPal'\n",
    "    ]\n",
    "\n",
    "    text_lower = enhanced_text.lower()\n",
    "\n",
    "    if re.search(financial_patterns['billions'], text_lower):\n",
    "        enhanced_text = \"[FINANCIAL_MAJOR] \" + enhanced_text\n",
    "    elif re.search(financial_patterns['millions'], text_lower):\n",
    "        enhanced_text = \"[FINANCIAL_MEDIUM] \" + enhanced_text\n",
    "    elif re.search(financial_patterns['thousands'], text_lower):\n",
    "        enhanced_text = \"[FINANCIAL_MINOR] \" + enhanced_text\n",
    "    if any(keyword in text_lower for keyword in regulatory_keywords):\n",
    "        enhanced_text = \"[REGULATORY_IMPACT] \" + enhanced_text\n",
    "    if any(keyword in text_lower for keyword in market_impact_keywords):\n",
    "        enhanced_text = \"[MARKET_IMPACT] \" + enhanced_text\n",
    "    if any(keyword in text_lower for keyword in security_keywords):\n",
    "        enhanced_text = \"[SECURITY_RISK] \" + enhanced_text\n",
    "    if any(entity in enhanced_text for entity in major_entities):\n",
    "        enhanced_text = \"[MAJOR_ENTITY] \" + enhanced_text\n",
    "\n",
    "    return enhanced_text\n",
    "\n",
    "def advanced_clean_text(text: str) -> str:\n",
    "    \"\"\"Enhanced text cleaning for crypto news.\"\"\"\n",
    "    if not text or pd.isna(text):\n",
    "        return \"\"\n",
    "\n",
    "    text = str(text)\n",
    "    text = re.sub(r'http[s]?://[^\\s]+', '[URL]', text)\n",
    "    text = re.sub(r'www\\.[^\\s]+', '[URL]', text)\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '[USER]', text)\n",
    "    text = re.sub(r'#([A-Za-z0-9_]+)', r'[HASHTAG] \\1', text)\n",
    "    text = re.sub(r'[^\\w\\s\\[\\]$%.,!?-]', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    if len(text) > 400:\n",
    "        text = text[:400]\n",
    "\n",
    "    return text\n",
    "\n",
    "def upload_and_load_1835_dataset():\n",
    "    print(\"CSV dosyanızı yükleyin:\")\n",
    "    uploaded = files.upload()\n",
    "\n",
    "    if not uploaded:\n",
    "        print(\"Dosya yüklenmedi!\")\n",
    "        return None\n",
    "\n",
    "    filename = list(uploaded.keys())[0]\n",
    "    print(f\"{filename} dosyası yüklendi\")\n",
    "\n",
    "    try:\n",
    "        encodings = ['utf-8', 'cp1252', 'iso-8859-1', 'latin-1']\n",
    "        df = None\n",
    "\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                df = pd.read_csv(filename, encoding=encoding)\n",
    "                print(f\"{encoding} encoding ile başarıyla yüklendi\")\n",
    "                break\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "\n",
    "        if df is None:\n",
    "            print(\"Hiçbir encoding ile okunamadı\")\n",
    "            return None\n",
    "\n",
    "        print(f\"Dataset shape: {df.shape}\")\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "        if 'expected_label' in df.columns:\n",
    "            df.rename(columns={'expected_label': 'label'}, inplace=True)\n",
    "\n",
    "        print(\"Veri temizleniyor...\")\n",
    "        df['text'] = df['text'].astype(str).apply(advanced_clean_text)\n",
    "        df = df[df['text'].str.strip() != '']\n",
    "        df['label'] = df['label'].astype(str).str.strip()\n",
    "\n",
    "        df = df.dropna(subset=['text', 'label'])\n",
    "\n",
    "        label_standardization = {\n",
    "            'important': 'Important',\n",
    "            'IMPORTANT': 'Important',\n",
    "            'Important': 'Important',\n",
    "            'medium': 'Medium',\n",
    "            'MEDIUM': 'Medium',\n",
    "            'Medium': 'Medium',\n",
    "            'unimportant': 'Unimportant',\n",
    "            'UNIMPORTANT': 'Unimportant',\n",
    "            'Unimportant': 'Unimportant'\n",
    "        }\n",
    "\n",
    "        df['label'] = df['label'].map(label_standardization).fillna(df['label'])\n",
    "\n",
    "        if config.ENHANCE_PATTERNS:\n",
    "            print(\"Pattern enhancement uygulanıyor...\")\n",
    "            df['text'] = df.apply(lambda row: enhance_text_with_patterns(row['text'], row['label']), axis=1)\n",
    "\n",
    "        print(f\"\\n1835 dataset etiket dağılımı:\")\n",
    "        label_counts = df['label'].value_counts()\n",
    "        for label, count in label_counts.items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        df['label_id'] = df['label'].map(config.LABEL_TO_ID)\n",
    "\n",
    "        df = df.dropna(subset=['label_id'])\n",
    "        df['label_id'] = df['label_id'].astype(int)\n",
    "\n",
    "        print(f\"\\n Final dataset: {len(df)} samples\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Dataset yükleme hatası: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_balanced_datasets(df):\n",
    "    \"\"\"Create perfectly balanced train/val/test splits - all classes equal to smallest class.\"\"\"\n",
    "    print(f\"\\n Creating PERFECTLY BALANCED datasets...\")\n",
    "\n",
    "    class_counts = df['label'].value_counts()\n",
    "    print(f\"\\n Current class distribution:\")\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\" {label}: {count} samples\")\n",
    "\n",
    "    min_class_size = class_counts.min()\n",
    "    print(f\"All classes will have exactly {min_class_size} samples\")\n",
    "\n",
    "    balanced_samples = []\n",
    "    for label in config.LABEL_NAMES:\n",
    "        label_samples = df[df['label'] == label]\n",
    "\n",
    "        if len(label_samples) >= min_class_size:\n",
    "            selected_samples = label_samples.sample(n=min_class_size, random_state=42)\n",
    "            print(f\"  {label}: {len(label_samples)} → {min_class_size} samples (downsampled)\")\n",
    "        else:\n",
    "            selected_samples = label_samples\n",
    "            print(f\"  {label}: {len(label_samples)} samples (all used)\")\n",
    "\n",
    "        balanced_samples.append(selected_samples)\n",
    "\n",
    "    df_balanced = pd.concat(balanced_samples, ignore_index=True)\n",
    "\n",
    "    final_counts = df_balanced['label'].value_counts()\n",
    "    print(f\"\\nPERFECTLY BALANCED distribution:\")\n",
    "    for label, count in final_counts.items():\n",
    "        percentage = (count / len(df_balanced)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    total_samples = len(df_balanced)\n",
    "    samples_per_class = total_samples // 3\n",
    "\n",
    "    if len(final_counts.unique()) == 1:\n",
    "        print(f\"All classes have exactly {final_counts.iloc[0]} samples\")\n",
    "        print(f\"Total balanced dataset: {total_samples} samples ({samples_per_class} per class)\")\n",
    "    else:\n",
    "        print(\"Something went wrong with balancing\")\n",
    "\n",
    "    print(f\"\\nShuffling balanced data...\")\n",
    "    df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nCreating stratified train/val/test split from balanced data...\")\n",
    "\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df_balanced,\n",
    "        test_size=0.30,\n",
    "        stratify=df_balanced['label_id'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df,\n",
    "        test_size=0.50,\n",
    "        stratify=temp_df['label_id'],\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    print(f\" Balanced split results:\")\n",
    "    print(f\"   Training: {len(train_df)} samples ({len(train_df)/len(df_balanced)*100:.1f}%)\")\n",
    "    print(f\"   Validation: {len(val_df)} samples ({len(val_df)/len(df_balanced)*100:.1f}%)\")\n",
    "    print(f\"   Test: {len(test_df)} samples ({len(test_df)/len(df_balanced)*100:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n Training set balance:\")\n",
    "    train_counts = train_df['label'].value_counts()\n",
    "    for label, count in train_counts.items():\n",
    "        percentage = (count / len(train_df)) * 100\n",
    "        print(f\"  {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "    print(f\"\\n Class weights: DISABLED (perfect balance achieved)\")\n",
    "    class_weights_dict = None\n",
    "\n",
    "    print(f\"\\n🔧 Setting up tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.BASE_MODEL_NAME)\n",
    "    print(f\" {config.BASE_MODEL_NAME} tokenizer loaded\")\n",
    "\n",
    "    if config.ENHANCE_PATTERNS:\n",
    "        special_tokens = [\n",
    "            \"[FINANCIAL_MAJOR]\", \"[FINANCIAL_MEDIUM]\", \"[FINANCIAL_MINOR]\",\n",
    "            \"[REGULATORY_IMPACT]\", \"[MARKET_IMPACT]\", \"[SECURITY_RISK]\",\n",
    "            \"[MAJOR_ENTITY]\", \"[URL]\", \"[USER]\", \"[HASHTAG]\"\n",
    "        ]\n",
    "\n",
    "        new_tokens = tokenizer.add_tokens(special_tokens)\n",
    "        if new_tokens > 0:\n",
    "            print(f\" Added {new_tokens} special pattern tokens\")\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=512,\n",
    "            add_special_tokens=True\n",
    "        )\n",
    "\n",
    "    train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']].reset_index(drop=True))\n",
    "    val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']].reset_index(drop=True))\n",
    "    test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']].reset_index(drop=True))\n",
    "\n",
    "    print(\"🔧 Tokenizing balanced datasets...\")\n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True, remove_columns=['text'])\n",
    "\n",
    "    train_dataset = train_dataset.rename_column('label_id', 'labels')\n",
    "    val_dataset = val_dataset.rename_column('label_id', 'labels')\n",
    "    test_dataset = test_dataset.rename_column('label_id', 'labels')\n",
    "\n",
    "    print(\"Perfectly balanced datasets created successfully\")\n",
    "    print(f\"Final result: {total_samples} samples, {samples_per_class} per class\")\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset, tokenizer, class_weights_dict\n",
    "\n",
    "def compute_advanced_metrics(eval_pred):\n",
    "    \"\"\"Compute detailed metrics for crypto classification.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(\n",
    "        labels, predictions, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    macro_f1 = f1.mean()\n",
    "    weighted_f1 = precision_recall_fscore_support(\n",
    "        labels, predictions, average='weighted', zero_division=0\n",
    "    )[2]\n",
    "\n",
    "    class_accuracy = {}\n",
    "    for i, label_name in enumerate(config.LABEL_NAMES):\n",
    "        mask = labels == i\n",
    "        if mask.sum() > 0:\n",
    "            class_accuracy[f'accuracy_{label_name}'] = (predictions[mask] == labels[mask]).mean()\n",
    "\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'macro_f1': macro_f1,\n",
    "        'weighted_f1': weighted_f1,\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'f1': f1.tolist()\n",
    "    }\n",
    "\n",
    "    metrics.update(class_accuracy)\n",
    "    return metrics\n",
    "\n",
    "class WeightedTrainer(Trainer):\n",
    "    \"\"\"Custom trainer with class weights for imbalanced dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "\n",
    "        if self.class_weights is not None:\n",
    "            weight_tensor = torch.tensor(\n",
    "                [self.class_weights[i] for i in range(len(self.class_weights))],\n",
    "                dtype=torch.float32,\n",
    "                device=labels.device\n",
    "            )\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def train_optimized_model():\n",
    "    \"\"\"Train model optimized for 1835 crypto news dataset.\"\"\"\n",
    "    print(\"OPTIMIZED TRAINING FOR CRYPTO NEWS DATASET\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df = upload_and_load_1835_dataset()\n",
    "    if df is None:\n",
    "        return None, None, None\n",
    "\n",
    "    train_dataset, val_dataset, test_dataset, tokenizer, class_weights = create_balanced_datasets(df)\n",
    "\n",
    "    print(f\"\\n Loading model: {config.BASE_MODEL_NAME}\")\n",
    "\n",
    "    from transformers import AutoConfig\n",
    "\n",
    "    model_config = AutoConfig.from_pretrained(config.BASE_MODEL_NAME)\n",
    "    print(f\" {config.BASE_MODEL_NAME} config loaded\")\n",
    "\n",
    "    model_config.num_labels = len(config.LABEL_NAMES)\n",
    "    model_config.id2label = config.ID_TO_LABEL\n",
    "    model_config.label2id = config.LABEL_TO_ID\n",
    "    model_config.hidden_dropout_prob = config.DROPOUT_RATE\n",
    "    model_config.attention_probs_dropout_prob = config.DROPOUT_RATE\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        config.BASE_MODEL_NAME,\n",
    "        config=model_config,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    print(f\" {config.BASE_MODEL_NAME} model loaded\")\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/content/training_output_1835\",\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        max_grad_norm=config.MAX_GRAD_NORM,\n",
    "        warmup_ratio=config.WARMUP_RATIO,\n",
    "        lr_scheduler_type=\"cosine_with_restarts\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_weighted_f1\",\n",
    "        greater_is_better=True,\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=100,\n",
    "        save_steps=100,\n",
    "        logging_steps=50,\n",
    "        save_total_limit=3,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "\n",
    "    trainer = WeightedTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer, padding=True),\n",
    "        compute_metrics=compute_advanced_metrics,\n",
    "        class_weights=class_weights,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
    "    )\n",
    "\n",
    "    device = get_device()\n",
    "    print(f\"\\n OPTIMIZED TRAINING CONFIGURATION:\")\n",
    "    print(f\"   Dataset size: 1835 samples\")\n",
    "    print(f\"   Model: {config.BASE_MODEL_NAME}\")\n",
    "    print(f\"   Batch size: {config.BATCH_SIZE}\")\n",
    "    print(f\"   Learning rate: {config.LEARNING_RATE}\")\n",
    "    print(f\"   Epochs: {config.NUM_EPOCHS}\")\n",
    "    print(f\"   Class weights: {'Enabled' if class_weights else 'Disabled'}\")\n",
    "    print(f\"   Pattern enhancement: {'Enabled' if config.ENHANCE_PATTERNS else 'Disabled'}\")\n",
    "    print(f\"   Device: {device}\")\n",
    "\n",
    "    print(f\"\\n Starting optimized training...\")\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\" Training completed successfully!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Training failed: {e}\")\n",
    "        print(\" Try reducing batch size or using a different model\")\n",
    "        return None, None, None\n",
    "\n",
    "    print(f\"\\n Evaluating on test set...\")\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "\n",
    "    print(f\"\\n Saving optimized model...\")\n",
    "    trainer.save_model(config.MODEL_SAVE_PATH)\n",
    "    tokenizer.save_pretrained(config.MODEL_SAVE_PATH)\n",
    "\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(f\" OPTIMIZED TRAINING COMPLETED\")\n",
    "    print(f\"=\" * 70)\n",
    "    print(f\" Model: {config.BASE_MODEL_NAME}\")\n",
    "    print(f\" Dataset: 1835 crypto news samples\")\n",
    "    print(f\" Test Accuracy: {test_results['eval_accuracy']:.4f}\")\n",
    "    print(f\" Test Macro F1: {test_results['eval_macro_f1']:.4f}\")\n",
    "    print(f\" Test Weighted F1: {test_results['eval_weighted_f1']:.4f}\")\n",
    "    print(f\" Test Loss: {test_results['eval_loss']:.4f}\")\n",
    "\n",
    "    accuracy = test_results['eval_accuracy']\n",
    "    if accuracy > 0.85:\n",
    "        print(f\"\\n EXCELLENT! Model ready for production\")\n",
    "    elif accuracy > 0.80:\n",
    "        print(f\"\\n VERY GOOD! Model performs well\")\n",
    "    elif accuracy > 0.75:\n",
    "        print(f\"\\n GOOD! Model is usable\")\n",
    "    else:\n",
    "        print(f\"\\n NEEDS IMPROVEMENT! Consider:\")\n",
    "\n",
    "    return trainer, tokenizer, test_dataset\n",
    "\n",
    "def detailed_evaluation(trainer, tokenizer, test_dataset):\n",
    "    \"\"\"Detailed evaluation with crypto-specific analysis.\"\"\"\n",
    "    if trainer is None:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n DETAILED EVALUATION\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "    y_true = test_dataset['labels']\n",
    "\n",
    "    report = classification_report(y_true, y_pred, target_names=config.LABEL_NAMES, output_dict=True)\n",
    "\n",
    "    print(f\" Class-wise performance:\")\n",
    "    for label in config.LABEL_NAMES:\n",
    "        if label in report:\n",
    "            print(f\"\\n{label}:\")\n",
    "            print(f\"  Precision: {report[label]['precision']:.4f}\")\n",
    "            print(f\"  Recall: {report[label]['recall']:.4f}\")\n",
    "            print(f\"  F1-score: {report[label]['f1-score']:.4f}\")\n",
    "            print(f\"  Support: {report[label]['support']}\")\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=config.LABEL_NAMES,\n",
    "                yticklabels=config.LABEL_NAMES)\n",
    "    plt.title(f'Confusion Matrix - {config.BASE_MODEL_NAME}\\n1835 Crypto News Dataset')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    confidence_scores = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=-1)\n",
    "    max_confidences = torch.max(confidence_scores, dim=-1)[0].numpy()\n",
    "\n",
    "    print(f\"\\n Confidence Analysis:\")\n",
    "    print(f\"  Average confidence: {np.mean(max_confidences):.3f}\")\n",
    "    print(f\"  Median confidence: {np.median(max_confidences):.3f}\")\n",
    "    print(f\"  Low confidence (<0.6): {np.sum(max_confidences < 0.6)} samples\")\n",
    "\n",
    "def test_with_new_examples(trainer, tokenizer):\n",
    "    \"\"\"Test model with new crypto news examples.\"\"\"\n",
    "    if trainer is None:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n TESTING WITH NEW EXAMPLES\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    test_examples = [\n",
    "        {\n",
    "            'text': \"TRUMP MEDIA BUYS $2 BILLION WORTH OF #BITCOIN AND BITCOIN-RELATED SECURITIES\",\n",
    "            'expected': 'Important'\n",
    "        },\n",
    "        {\n",
    "            'text': \"STRATEGY BUYS ANOTHER 6,220 #BITCOIN FOR $739.8 MILLION\",\n",
    "            'expected': 'Important'\n",
    "        },\n",
    "        {\n",
    "            'text': \"$763 million #Bitcoin treasury company Nakamoto files with SEC to merge with publicly traded KindlyMD. Expects to complete the merge in 20 days.\",\n",
    "            'expected': 'Unimportant'\n",
    "        },\n",
    "        {\n",
    "            'text': \"President Trump is preparing to open the $9 trillion US retirement market to Bitcoin & crypto investments — Financial Times\",\n",
    "            'expected': 'Important'\n",
    "        },\n",
    "        {\n",
    "            'text': \"White House says it is exploring de minimis tax exemption for Bitcoin 'to make crypto payments easier.'\",\n",
    "            'expected': 'Important'\n",
    "        },\n",
    "        {\n",
    "            'text': \"Congressman Brad Sherman says 'crypto has all the power in Congress.'\",\n",
    "            'expected': 'Unimportant'\n",
    "        },\n",
    "        {\n",
    "            'text': \"Thailand to launch a crypto sandbox to allow foreign tourists to use #Bitcoin and crypto in Thailand. 2k\",\n",
    "            'expected': 'Medium'\n",
    "        },\n",
    "        {\n",
    "            'text': \"US Marshalls reveal the government now only holds 28,988 Bitcoin worth $3.4 billion, instead of the estimated ~200,000 BTC\",\n",
    "            'expected': 'Important'\n",
    "        },\n",
    "        {\n",
    "            'text': \"$39,600 is the favored short-term target, whether or not Bitcoin price action ultimately returns to downward momentum.\",\n",
    "            'expected': 'Medium'\n",
    "        },\n",
    "        {\n",
    "            'text': \"The central People's Bank of China (PBoC)'s digital yuan is set to expand into the world of wealth management – and has also broken new ground in the field of education.\",\n",
    "            'expected': 'Important'\n",
    "        },\n",
    "        {\n",
    "            'text': \"A new report conducted by Tecnalia Research and Chainlink Labs asserts that blockchain and oracles can help fix climate issues.\",\n",
    "            'expected': 'Medium'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    device = get_device()\n",
    "    model = trainer.model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(\" Test Results:\")\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i, example in enumerate(test_examples, 1):\n",
    "        if config.ENHANCE_PATTERNS:\n",
    "            enhanced_text = enhance_text_with_patterns(example['text'])\n",
    "        else:\n",
    "            enhanced_text = example['text']\n",
    "\n",
    "        clean_text = advanced_clean_text(enhanced_text)\n",
    "\n",
    "        inputs = tokenizer(clean_text, truncation=True, padding=True,\n",
    "                         max_length=512, return_tensors=\"pt\")\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\n",
    "        pred_id = torch.argmax(predictions[0]).item()\n",
    "        confidence = predictions[0][pred_id].item()\n",
    "        predicted = config.ID_TO_LABEL[pred_id]\n",
    "\n",
    "        is_correct = predicted == example['expected']\n",
    "        if is_correct:\n",
    "            correct_predictions += 1\n",
    "\n",
    "        status = \"✅\" if is_correct else \"❌\"\n",
    "        print(f\"\\n{i}. {status}\")\n",
    "        print(f\"   Text: {example['text'][:80]}...\")\n",
    "        print(f\"   Expected: {example['expected']}\")\n",
    "        print(f\"   Predicted: {predicted} (confidence: {confidence:.3f})\")\n",
    "\n",
    "        if config.ENHANCE_PATTERNS and enhanced_text != example['text']:\n",
    "            print(f\"   Enhanced: {enhanced_text[:80]}...\")\n",
    "\n",
    "    accuracy = correct_predictions / len(test_examples)\n",
    "    print(f\"\\n New Examples Test Results:\")\n",
    "    print(f\"   Accuracy: {correct_predictions}/{len(test_examples)} ({accuracy*100:.1f}%)\")\n",
    "\n",
    "def fine_tune_model(trainer, tokenizer, test_dataset):\n",
    "    \"\"\"Fine-tune the model for better performance.\"\"\"\n",
    "    if trainer is None:\n",
    "        return None\n",
    "\n",
    "    print(f\"\\n FINE-TUNING MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # fine-tuning için veri setini yeniden yükle\n",
    "    print(\" Re-loading dataset for fine-tuning...\")\n",
    "    df = upload_and_load_1835_dataset()\n",
    "    if df is None:\n",
    "        return None\n",
    "\n",
    "    # fine-tuning için gelişmiş desenler uygulayın\n",
    "    def apply_enhanced_patterns(text, label):\n",
    "        enhanced = text\n",
    "        text_lower = text.lower()\n",
    "\n",
    "        if re.search(r'\\$\\d+\\.?\\d*\\s*billion', text_lower):\n",
    "            enhanced = \"[CRITICAL_IMP] \" + enhanced\n",
    "        elif re.search(r'\\$\\d+\\.?\\d*\\s*million', text_lower):\n",
    "            enhanced = \"[MEDIUM_IMP] \" + enhanced\n",
    "        elif any(word in text_lower for word in ['SEC', 'government', 'regulatory']):\n",
    "            enhanced = \"[HIGH_IMP] \" + enhanced\n",
    "        elif any(word in text_lower for word in ['hack', 'exploit', 'breach']):\n",
    "            enhanced = \"[SECURITY] \" + enhanced\n",
    "        else:\n",
    "            enhanced = \"[LOW_PATTERN] \" + enhanced\n",
    "\n",
    "        return enhanced\n",
    "\n",
    "    print(\" Applying enhanced patterns for fine-tuning...\")\n",
    "    df['text'] = df.apply(lambda row: apply_enhanced_patterns(row['text'], row['label']), axis=1)\n",
    "\n",
    "    # yeni dengeli veri kümeleri oluşturun\n",
    "    train_dataset_ft, val_dataset_ft, test_dataset_ft, tokenizer_ft, _ = create_balanced_datasets(df)\n",
    "\n",
    "    new_tokens = [\"[CRITICAL_IMP]\", \"[HIGH_IMP]\", \"[MEDIUM_IMP]\", \"[SECURITY]\", \"[LOW_PATTERN]\"]\n",
    "    added_tokens = tokenizer_ft.add_tokens(new_tokens)\n",
    "    print(f\" Added {added_tokens} new tokens for fine-tuning\")\n",
    "\n",
    "    trainer.model.resize_token_embeddings(len(tokenizer_ft))\n",
    "\n",
    "    # Fine-tuning argümanları\n",
    "    fine_tune_args = TrainingArguments(\n",
    "        output_dir=\"/content/fine_tuned_output\",\n",
    "        num_train_epochs=3,  # daha az epochs\n",
    "        per_device_train_batch_size=max(1, config.BATCH_SIZE // 2),  # küçük batch\n",
    "        per_device_eval_batch_size=max(1, config.BATCH_SIZE // 2),\n",
    "        learning_rate=5e-6,  # düşük learning rate\n",
    "        weight_decay=0.01,\n",
    "        warmup_ratio=0.1,\n",
    "\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_weighted_f1\",\n",
    "        greater_is_better=True,\n",
    "\n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        save_steps=50,\n",
    "        logging_steps=25,\n",
    "        save_total_limit=2,\n",
    "\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        dataloader_num_workers=0,\n",
    "        remove_unused_columns=False,\n",
    "        report_to=[],\n",
    "\n",
    "        seed=42,\n",
    "        data_seed=42,\n",
    "    )\n",
    "\n",
    "    fine_tune_trainer = WeightedTrainer(\n",
    "        model=trainer.model,\n",
    "        args=fine_tune_args,\n",
    "        train_dataset=train_dataset_ft,\n",
    "        eval_dataset=val_dataset_ft,\n",
    "        tokenizer=tokenizer_ft,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer_ft, padding=True),\n",
    "        compute_metrics=compute_advanced_metrics,\n",
    "        class_weights=None,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "    )\n",
    "\n",
    "    print(f\" Fine-tuning Configuration:\")\n",
    "    print(f\"   Learning rate: {fine_tune_args.learning_rate}\")\n",
    "    print(f\"   Epochs: {fine_tune_args.num_train_epochs}\")\n",
    "    print(f\"   Batch size: {fine_tune_args.per_device_train_batch_size}\")\n",
    "\n",
    "    try:\n",
    "        print(\" Starting fine-tuning...\")\n",
    "        fine_tune_trainer.train()\n",
    "        print(\" Fine-tuning completed successfully!\")\n",
    "\n",
    "        print(\" Evaluating fine-tuned model...\")\n",
    "        ft_results = fine_tune_trainer.evaluate(test_dataset_ft)\n",
    "\n",
    "        print(f\"\\n FINE-TUNING RESULTS:\")\n",
    "        print(f\"   Accuracy: {ft_results['eval_accuracy']:.4f}\")\n",
    "        print(f\"   Macro F1: {ft_results['eval_macro_f1']:.4f}\")\n",
    "        print(f\"   Weighted F1: {ft_results['eval_weighted_f1']:.4f}\")\n",
    "\n",
    "        return fine_tune_trainer, tokenizer_ft\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Fine-tuning failed: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def save_model_for_production(trainer, tokenizer):\n",
    "    \"\"\"Save model with production-ready format for Colab.\"\"\"\n",
    "    if trainer is None:\n",
    "        return\n",
    "\n",
    "    print(f\"\\n SAVING PRODUCTION MODEL\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    production_path = \"/content/crypto_model_production\"\n",
    "    os.makedirs(production_path, exist_ok=True)\n",
    "\n",
    "    trainer.save_model(production_path)\n",
    "    tokenizer.save_pretrained(production_path)\n",
    "\n",
    "    config_dict = {\n",
    "        'model_name': config.BASE_MODEL_NAME,\n",
    "        'num_labels': len(config.LABEL_NAMES),\n",
    "        'label_names': config.LABEL_NAMES,\n",
    "        'label_to_id': config.LABEL_TO_ID,\n",
    "        'id_to_label': config.ID_TO_LABEL,\n",
    "        'enhance_patterns': config.ENHANCE_PATTERNS,\n",
    "        'training_samples': 1835,\n",
    "        'batch_size': config.BATCH_SIZE,\n",
    "        'learning_rate': config.LEARNING_RATE,\n",
    "        'epochs': config.NUM_EPOCHS\n",
    "    }\n",
    "\n",
    "    with open(f\"{production_path}/training_config.json\", 'w') as f:\n",
    "        json.dump(config_dict, f, indent=2)\n",
    "\n",
    "    zip_path = '/content/crypto_model.zip'\n",
    "    print(f\" Model oluşturuluyor...\")\n",
    "\n",
    "    try:\n",
    "        with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "            for root, dirs, files_list in os.walk(production_path):\n",
    "                for file in files_list:\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    arcname = os.path.relpath(file_path, production_path)\n",
    "                    zipf.write(file_path, f\"crypto_model/{arcname}\")\n",
    "\n",
    "        files.download(zip_path)\n",
    "        print(f\"Model  indirildi!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Model indirme hatası: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main execution function for 1835 dataset training.\"\"\"\n",
    "    print(\"Optimized for Real-World Crypto News Classification\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Optimize edilmiş modeli eğitin\n",
    "    trainer, tokenizer, test_dataset = train_optimized_model()\n",
    "\n",
    "    if trainer is not None:\n",
    "        detailed_evaluation(trainer, tokenizer, test_dataset)\n",
    "\n",
    "        test_with_new_examples(trainer, tokenizer)\n",
    "\n",
    "        fine_tune_choice = input(f\"\\n Do you want to fine-tune the model? (y/n): \")\n",
    "        if fine_tune_choice.lower() == 'y':\n",
    "            fine_tuned_trainer, fine_tuned_tokenizer = fine_tune_model(trainer, tokenizer, test_dataset)\n",
    "\n",
    "            if fine_tuned_trainer is not None:\n",
    "                print(\"\\nTesting fine-tuned model with new examples...\")\n",
    "                test_with_new_examples(fine_tuned_trainer, fine_tuned_tokenizer)\n",
    "\n",
    "                trainer = fine_tuned_trainer\n",
    "                tokenizer = fine_tuned_tokenizer\n",
    "\n",
    "        save_choice = input(f\"\\nSave model for production use? (y/n): \")\n",
    "        if save_choice.lower() == 'y':\n",
    "            save_model_for_production(trainer, tokenizer)\n",
    "\n",
    "        print(f\"\\nTRAINING COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "    else:\n",
    "        print(f\"Training failed!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
